{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a notebook with relevant scripts to transfrom and query data on SAC PM influence Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run block to Install pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: sentence_transformers in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from sentence_transformers) (4.41.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from sentence_transformers) (4.66.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from sentence_transformers) (2.3.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from sentence_transformers) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from sentence_transformers) (1.5.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from sentence_transformers) (1.14.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from sentence_transformers) (0.23.4)\n",
      "Requirement already satisfied: Pillow in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from sentence_transformers) (10.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2024.6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.25.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (1.12.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (2021.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from tqdm->sentence_transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (2024.5.15)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.4.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.11.0->sentence_transformers) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.11.0->sentence_transformers) (2021.13.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.6.2)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: flupy in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (4.66.4)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from flupy) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: supabase in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: gotrue<3.0,>=1.3 in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from supabase) (2.5.4)\n",
      "Requirement already satisfied: httpx<0.28,>=0.24 in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from supabase) (0.27.0)\n",
      "Requirement already satisfied: postgrest<0.17.0,>=0.14 in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from supabase) (0.16.8)\n",
      "Requirement already satisfied: realtime<2.0.0,>=1.0.0 in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from supabase) (1.0.6)\n",
      "Requirement already satisfied: storage3<0.8.0,>=0.5.3 in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from supabase) (0.7.6)\n",
      "Requirement already satisfied: supafunc<0.5.0,>=0.3.1 in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from supabase) (0.4.6)\n",
      "Requirement already satisfied: pydantic<3,>=1.10 in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from gotrue<3.0,>=1.3->supabase) (2.7.4)\n",
      "Requirement already satisfied: anyio in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from httpx<0.28,>=0.24->supabase) (4.4.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from httpx<0.28,>=0.24->supabase) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from httpx<0.28,>=0.24->supabase) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from httpx<0.28,>=0.24->supabase) (2.10)\n",
      "Requirement already satisfied: sniffio in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from httpx<0.28,>=0.24->supabase) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from httpcore==1.*->httpx<0.28,>=0.24->supabase) (0.14.0)\n",
      "Requirement already satisfied: deprecation<3.0.0,>=2.1.0 in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from postgrest<0.17.0,>=0.14->supabase) (2.1.0)\n",
      "Requirement already satisfied: strenum<0.5.0,>=0.4.9 in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from postgrest<0.17.0,>=0.14->supabase) (0.4.15)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.1 in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from realtime<2.0.0,>=1.0.0->supabase) (2.9.0)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.12.2 in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from realtime<2.0.0,>=1.0.0->supabase) (4.12.2)\n",
      "Requirement already satisfied: websockets<13,>=11 in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from realtime<2.0.0,>=1.0.0->supabase) (12.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from deprecation<3.0.0,>=2.1.0->postgrest<0.17.0,>=0.14->supabase) (24.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from pydantic<3,>=1.10->gotrue<3.0,>=1.3->supabase) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from pydantic<3,>=1.10->gotrue<3.0,>=1.3->supabase) (2.18.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from python-dateutil<3.0.0,>=2.8.1->realtime<2.0.0,>=1.0.0->supabase) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: vecs in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (0.4.3)\n",
      "Requirement already satisfied: pgvector==0.1.* in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from vecs) (0.1.8)\n",
      "Requirement already satisfied: sqlalchemy==2.* in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from vecs) (2.0.31)\n",
      "Requirement already satisfied: psycopg2-binary==2.9.* in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from vecs) (2.9.9)\n",
      "Requirement already satisfied: flupy==1.* in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from vecs) (1.2.0)\n",
      "Requirement already satisfied: deprecated==1.2.* in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from vecs) (1.2.14)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from deprecated==1.2.*->vecs) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from flupy==1.*->vecs) (4.12.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from pgvector==0.1.*->vecs) (1.26.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from sqlalchemy==2.*->vecs) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting ipywidgets\n",
      "  Using cached ipywidgets-8.1.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from ipywidgets) (8.25.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.11 (from ipywidgets)\n",
      "  Using cached widgetsnbextension-4.0.11-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab-widgets~=3.0.11 (from ipywidgets)\n",
      "  Using cached jupyterlab_widgets-3.0.11-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: decorator in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.47)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: stack-data in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\i586445\\appdata\\local\\anaconda3\\envs\\vecs\\lib\\site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Using cached ipywidgets-8.1.3-py3-none-any.whl (139 kB)\n",
      "Using cached jupyterlab_widgets-3.0.11-py3-none-any.whl (214 kB)\n",
      "Using cached widgetsnbextension-4.0.11-py3-none-any.whl (2.3 MB)\n",
      "Installing collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.3 jupyterlab-widgets-3.0.11 widgetsnbextension-4.0.11\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas\n",
    "%pip install numpy\n",
    "%pip install sentence_transformers\n",
    "%pip install flupy tqdm python-dotenv\n",
    "%pip install supabase\n",
    "%pip install vecs\n",
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import modules\n",
    "\n",
    "Supabase Credentials are saved as Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\I586445\\AppData\\Local\\anaconda3\\envs\\vecs\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import vecs\n",
    "from flupy import flu\n",
    "from tqdm import tqdm\n",
    "from typing import List, Tuple, Dict\n",
    "import numpy as np\n",
    "\n",
    "import supabase\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import ast\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to clean data from https://sapit-iex-prod-lizard-customer-influence-workbench-ciw-6f02fb4d.cfapps.eu10-004.hana.ondemand.com/cccissessionowner/index.html#/DetailView/293,I311255\n",
    "\n",
    "Change CSV path and run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def clean_data(df):\n",
    "    # Drop columns: 'FBA Comments', 'FBA Status', 'User ID', 'Internal Project ID'\n",
    "    df = df.drop(columns=['FBA Comments', 'FBA Status', 'User ID', 'Internal Project ID'])\n",
    "    # Drop columns: 'Is Submitter ?', 'ERP Customer No.', 'Account Type', 'Voter Zip Code', 'Voter City', 'Voter Street', 'Voter Email', 'Voter Name'\n",
    "    df = df.drop(columns=['Is Submitter ?', 'ERP Customer No.', 'Account Type', 'Voter Zip Code', 'Voter City', 'Voter Street', 'Voter Email', 'Voter Name'])\n",
    "    # Drop columns: 'INM Project Name', 'Coach Name', 'Category'\n",
    "    df = df.drop(columns=['INM Project Name', 'Coach Name', 'Category'])\n",
    "    # Drop columns: 'IR Link', 'IR Title', 'Decision Reason', 'Reference'\n",
    "    df = df.drop(columns=['IR Link', 'IR Title', 'Decision Reason', 'Reference'])\n",
    "    # Drop columns: 'Views', 'Customer Votes', 'Last Status Change Date', 'Status', 'Reason Comments'\n",
    "    df = df.drop(columns=['Views', 'Customer Votes', 'Last Status Change Date', 'Status', 'Reason Comments'])\n",
    "    # Change column type to datetime64[ns] for column: 'Voted Date'\n",
    "    df = df.astype({'Voted Date': 'datetime64[ns]'})\n",
    "    # Change column type to string for column: 'Voter Company'\n",
    "    df = df.astype({'Voter Company': 'string'})\n",
    "    # Drop duplicate rows in columns: 'IR ID', 'Voter Company'\n",
    "    df = df.drop_duplicates(subset=['IR ID', 'Voter Company'])\n",
    "    # Change column type to category for column: 'Voter Company'\n",
    "    df = df.astype({'Voter Company': 'category'})\n",
    "    # Change column type to string for column: 'ir id'\n",
    "    df = df.astype({'ir id': 'string'})\n",
    "    return df\n",
    "\n",
    "# Loaded variable 'df' from URI: \n",
    "df = pd.read_csv(r'c:\\\\Users\\\\I586445\\\\OneDrive - National University of Ireland, Galway\\\\Documents\\\\InfluenceScripts\\\\Copy of Improvements_and_Votes(18-06-2024, 13_35).csv')\n",
    "\n",
    "df = clean_data(df)\n",
    "#save to csv\n",
    "df.to_csv(r'c:\\\\Users\\\\I586445\\\\OneDrive - National University of Ireland, Galway\\\\Documents\\\\InfluenceScripts\\\\IR_Votes_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to create consumer Foreign keys to connect Jira to influence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code executed successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Specify the path to your CSV file\n",
    "csv_file_path = r\"C:\\Users\\I586445\\Downloads\\17-06 Model Update\\FPA34 17_06.csv\"\n",
    "# Construct the output file path\n",
    "output_folder = os.path.dirname(csv_file_path)\n",
    "output_file_path = os.path.join(output_folder, \"Consumer Foreign keys.csv\")\n",
    "\n",
    "# Open the CSV file for reading and the output file for writing\n",
    "with open(csv_file_path, 'r', encoding='utf-8-sig') as file, open(output_file_path, 'w', newline='', encoding='utf-8') as output_file:\n",
    "    # Create a CSV reader object\n",
    "    csv_reader = csv.reader(file)\n",
    "    # Create a CSV writer object\n",
    "    csv_writer = csv.writer(output_file)\n",
    "    \n",
    "    # Write the header row with selected columns\n",
    "    csv_writer.writerow([\"ISSUEKEY\", \"CONSUMER\"])\n",
    "    \n",
    "    # Process the header row to get the column indices\n",
    "    header = next(csv_reader)\n",
    "    column_indices = {}\n",
    "    for column_name in [\"ISSUEKEY\", \"CONSUMER\"]:\n",
    "        try:\n",
    "            column_indices[column_name] = header.index(column_name)\n",
    "        except ValueError:\n",
    "            # Handle the case if any of the required columns are not found in the header\n",
    "            print(f\"Column '{column_name}' not found in the CSV file.\")\n",
    "            exit(1)\n",
    "\n",
    "    # Process each row and write selected columns to the output file\n",
    "    for row in csv_reader:\n",
    "        issue_key = row[column_indices[\"ISSUEKEY\"]]\n",
    "        consumer_digits = row[column_indices[\"CONSUMER\"]]\n",
    "        \n",
    "    \n",
    "        # Extract the \"Consumer Digits\" from the \"Custom field (Consumer)\" column using regex\n",
    "        matches_consumer = re.findall(r\"\\bhttps://influence.sap.com.*?(\\d{6})\", consumer_digits)\n",
    "        \n",
    "        # If matches are found, create a new row for each match\n",
    "        if matches_consumer:\n",
    "            for match in matches_consumer:\n",
    "                # Create a new row with details from the original row and the corresponding match\n",
    "                new_row = [issue_key, match]\n",
    "                csv_writer.writerow(new_row)\n",
    "        else:\n",
    "            # If no matches found, create a new row with details from the original row and an empty \"Consumer Digits\"\n",
    "            new_row = [issue_key, \"\"]\n",
    "            csv_writer.writerow(new_row)\n",
    "\n",
    "print(\"Code executed successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Searching for explicit mention of text in the descrtipon field of influence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the CSV files\n",
    "Influence_CSV = r\"C:\\Users\\I586445\\OneDrive - National University of Ireland, Galway\\Documents\\InfluenceScripts\\simlarity\\SAP Analytics Cloud & SAP Digital Boardroom.csv\"\n",
    "# Initialize influence to None or an empty DataFrame\n",
    "influence = None\n",
    "\n",
    "# Load CSV files into Pandas DataFrames\n",
    "try:\n",
    "    influence = pd.read_csv(Influence_CSV, usecols=['Request_ID', 'Idea_Title', 'Description', 'Status','Voting Score'])\n",
    "except Exception as e:\n",
    "    print(f\"Error reading the CSV file: {e}\")\n",
    "\n",
    "# Proceed only if influence is not None\n",
    "if influence is not None:\n",
    "    # Search for text in the description column LIKE% BW Live or BW or LIVE or Business Warehouse\n",
    "    search = influence[influence['Description'].str.contains('BW Live|BW|Business Warehouse', case=False, na=False)]\n",
    "\n",
    "    # Save the filtered data to a new CSV file\n",
    "    search.to_csv('search.csv', index=False)\n",
    "\n",
    "    print(\"Data search completed successfully!\")\n",
    "else:\n",
    "    print(\"Failed to load data. Please check the CSV file and column names.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This  snippet reads data from a CSV file, checks each row for the presence of the string \"SAC_PM_ACTIVE\", and writes selected data along with the check result to a new CSV file. \n",
    "\n",
    "It specifically extracts \"Issue key\" and \"Issue id\" from the input, adds a boolean indicating the presence of \"SAC_PM_ACTIVE\", and leaves \"Consumer Digits\" empty in the output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Specify the path to your CSV file\n",
    "csv_file_path = r\"C:\\Users\\I586445\\Downloads\\model refresh\\SAPJIRA without influence.csv\"\n",
    "# Construct the output file path\n",
    "output_folder = os.path.dirname(csv_file_path)\n",
    "output_file_path = os.path.join(output_folder, \"SAC_PM_ACTIVE_NO_INFLUENCE.csv\")\n",
    "\n",
    "# Open the CSV file for reading and the output file for writing\n",
    "with open(csv_file_path, 'r', encoding='utf-8') as file, open(output_file_path, 'w', newline='', encoding='utf-8') as output_file:\n",
    "    # Create a CSV reader object\n",
    "    csv_reader = csv.reader(file)\n",
    "    # Create a CSV writer object\n",
    "    csv_writer = csv.writer(output_file)\n",
    "    \n",
    "    # Write the header row with selected columns\n",
    "    csv_writer.writerow([\"Issue key\", \"Issue id\", \"SAC_PM_ACTIVE\", \"Consumer Digits\"])\n",
    "    \n",
    "    # Process the header row to get the column indices\n",
    "    header = next(csv_reader)\n",
    "    column_indices = {}\n",
    "    for column_name in [\"Issue key\", \"Issue id\"]:\n",
    "        try:\n",
    "            column_indices[column_name] = header.index(column_name)\n",
    "        except ValueError:\n",
    "            # Handle the case if any of the required columns are not found in the header\n",
    "            print(f\"Column '{column_name}' not found in the CSV file.\")\n",
    "            exit(1)\n",
    "\n",
    "    # Process each row and write selected columns to the output file\n",
    "    for row in csv_reader:\n",
    "        issue_key = row[column_indices[\"Issue key\"]]\n",
    "        issue_id = row[column_indices[\"Issue id\"]]\n",
    "    \n",
    "        \n",
    "        # Check if \"SAC_PM_ACTIVE\" is found in any field of the row\n",
    "        sac_pm_active = any(\"SAC_PM_ACTIVE\" in field for field in row)\n",
    "        \n",
    "        new_row = [issue_key, issue_id, sac_pm_active, \"\"]\n",
    "        csv_writer.writerow(new_row)\n",
    "        \n",
    "\n",
    "print(\"Code executed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to analyse influence requests via cosine Simlarity using Pgvector from supabase \n",
    "\n",
    "This requires supabase account \n",
    "https://supabase.com/docs/guides/ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting Requests to Vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the database connection string from the environment variables\n",
    "DB_CONNECTION = os.getenv(\"DB_CONNECTION\")\n",
    "\n",
    "# Create vector store client\n",
    "vx = vecs.create_client(DB_CONNECTION)\n",
    "print(\"Connected to vector store\")\n",
    "\n",
    "# Create a PostgreSQL/pgvector table named \"product_requests\" to contain the request embeddings\n",
    "product_requests = vx.get_or_create_collection(name=\"product_requests\", dimension=384)\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file = r\"C:\\Users\\I586445\\Downloads\\improvement request_12-06-2024_11-48\\SAP Analytics Cloud & SAP Digital Boardroom.csv\"\n",
    "\n",
    "def preprocess_description(description: str) -> str:\n",
    "    # Sentences to exclude\n",
    "    sentences_to_exclude = [\n",
    "        \"Please describe your improvement request\",\n",
    "        \"What is the opportunity/problem the request will address?\",\n",
    "        \"What is the expected benefit?\"\n",
    "    ]\n",
    "\n",
    "    # Remove the sentences\n",
    "    for sentence in sentences_to_exclude:\n",
    "        description = description.replace(sentence, \"\").strip()\n",
    "    return description\n",
    "# Read the CSV file and extract the product feature requests\n",
    "requests = []\n",
    "with open(csv_file, 'r', encoding='utf-8-sig') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        request_id = row[0]  # Assuming the request id is in the first column\n",
    "        description = row[16]  # Assuming the description is in the second column\n",
    "        requests.append((request_id, description))\n",
    "\n",
    "# Initialize the SentenceTransformer model\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "batch_size = 50\n",
    "records: List[Tuple[str, np.ndarray, Dict]] = []\n",
    "\n",
    "# Process the dataset in chunks\n",
    "for chunk_ix, chunk in tqdm(enumerate(flu(requests).chunk(batch_size)), desc=\"Processing chunks\"):\n",
    "    # Extract just the descriptions from the current chunk\n",
    "    chunk_descriptions = [preprocess_description(description) for _, description in chunk]\n",
    "    # Create embeddings for the current chunk\n",
    "    embedding_chunk = model.encode(chunk_descriptions)\n",
    "\n",
    "    # Enumerate the embeddings and create a record to insert into the database\n",
    "    for row_ix, embedding in enumerate(embedding_chunk):\n",
    "        actual_request_id = chunk[row_ix][0]  # Get the actual request_id from the chunk\n",
    "        records.append((actual_request_id, embedding, {\"text\": chunk_descriptions[row_ix]}))\n",
    "\n",
    "# Insert the records into the collection\n",
    "for record in tqdm(records, desc=\"Inserting records\"):\n",
    "    product_requests.upsert([record])\n",
    "\n",
    "print(\"All product feature requests have been embedded and inserted into the collection.\")\n",
    "\n",
    "product_requests.create_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compares Vectors by \n",
    "connecting to a Supabase database, fetches vector data, calculates the cosine similarity between vectors, filters results based on a similarity threshold, and saves the filtered data to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Replace with your Supabase connection details\n",
    "SUPABASE_URL = os.getenv('SUPABASE_URL')\n",
    "SUPABASE_KEY = os.getenv('SUPABASE_KEY')\n",
    "\n",
    "# Define the Supabase client\n",
    "supabase_client = supabase.create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "print(\"Connected to Supabase\")\n",
    "\n",
    "# Fetch all data from Supabase in one call\n",
    "try:\n",
    "    response = supabase_client.table('new').select('id', 'vec').execute()\n",
    "    data = response.data\n",
    "    print(f\"Fetched {len(data)} rows from Supabase\")\n",
    "except Exception as e:\n",
    "    print(f\"Error fetching data from Supabase: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Extract vectors and IDs\n",
    "ids = [item['id'] for item in data]\n",
    "vectors = [item['vec'] for item in data]\n",
    "\n",
    "# Convert vectors from strings to lists of floats\n",
    "try:\n",
    "    vectors = [ast.literal_eval(vec) for vec in vectors]\n",
    "    print(\"Vectors converted from strings to lists\")\n",
    "except Exception as e:\n",
    "    print(f\"Error converting vectors from strings to lists: {e}\")\n",
    "\n",
    "# Convert vectors to numpy array\n",
    "try:\n",
    "    vectors = np.array(vectors, dtype=float)\n",
    "    print(\"Vectors converted to numpy array\")\n",
    "except Exception as e:\n",
    "    print(f\"Error converting vectors to numpy array: {e}\")\n",
    "\n",
    "# Calculate cosine similarity matrix\n",
    "try:\n",
    "    similarity_matrix = cosine_similarity(vectors)\n",
    "    print(\"Cosine similarity matrix calculated\")\n",
    "except Exception as e:\n",
    "    print(f\"Error calculating cosine similarity: {e}\")\n",
    "\n",
    "# Minimum similarity threshold (in percentage)\n",
    "MIN_SIMILARITY_THRESHOLD = 70\n",
    "\n",
    "# Prepare the data for CSV with similarity percentages and apply threshold\n",
    "csv_data = []\n",
    "for idx, feature_id in enumerate(ids):\n",
    "    similarity_scores = similarity_matrix[idx]\n",
    "    top_indices = similarity_scores.argsort()[-6:-1][::-1]  # Exclude self and get top 5\n",
    "    for i in top_indices:\n",
    "        similarity_percentage = (similarity_scores[i] + 1) / 2 * 100\n",
    "        if similarity_percentage >= MIN_SIMILARITY_THRESHOLD:\n",
    "            csv_data.append({\n",
    "                'Feature ID': feature_id,\n",
    "                'Similar Feature ID': ids[i],\n",
    "                'Similarity Percentage': similarity_percentage\n",
    "            })\n",
    "    print(f\"Top similar features for feature ID {feature_id} found and filtered by similarity threshold\")\n",
    "\n",
    "# Create DataFrame and save to CSV\n",
    "df = pd.DataFrame(csv_data)\n",
    "df.to_csv('full_similar_product_features.csv', index=False)\n",
    "print(\"CSV file 'full_similar_product_features.csv' created successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge similar requests with external Metadata from influence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Paths to the CSV files\n",
    "full_similar_features_csv = 'similar_product_features.csv'\n",
    "sap_analytics_csv = 'SAP Analytics Cloud & SAP Digital Boardroom.csv'\n",
    "\n",
    "try:\n",
    "    # Load CSV files into Pandas DataFrames\n",
    "    full_similar_features = pd.read_csv(full_similar_features_csv)\n",
    "    sap_analytics = pd.read_csv(sap_analytics_csv, usecols=['Request_ID', 'Idea_Title', 'Description','Status'])\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: One or more CSV files not found. Please check the file paths.\")\n",
    "    exit(1)\n",
    "except pd.errors.ParserError:\n",
    "    print(f\"Error: Failed to parse the CSV files. Please check the file format.\")\n",
    "    exit(1)\n",
    "\n",
    "try:\n",
    "    # Merge the metadata for both feature_id and similar_feature_id\n",
    "    merged_data = pd.merge(full_similar_features, sap_analytics, left_on='Feature_ID', right_on='Request_ID')\n",
    "    merged_data = pd.merge(merged_data, sap_analytics, left_on='Similar_Feature_ID', right_on='Request_ID')\n",
    "except KeyError:\n",
    "    print(f\"Error: One or more columns not found in the CSV files. Please check the column names.\")\n",
    "    exit(1)\n",
    "\n",
    "try:\n",
    "    # Drop the duplicate columns\n",
    "    merged_data = merged_data.drop_duplicates(subset=['Feature_ID', 'Similar_Feature_ID'], keep='first')\n",
    "except ValueError:\n",
    "    print(f\"Error: Failed to drop duplicate rows. Please check the data.\")\n",
    "    exit(1)\n",
    "\n",
    "try:\n",
    "    # Save the merged data to a new CSV file\n",
    "    merged_data.to_csv('merged_data.csv', index=False)\n",
    "except PermissionError:\n",
    "    print(f\"Error: Failed to write to the output CSV file. Please check the file permissions.\")\n",
    "    exit(1)\n",
    "except IOError:\n",
    "    print(f\"Error: Failed to write to the output CSV file. Please check the file path.\")\n",
    "    exit(1)\n",
    "\n",
    "print(\"Data merging completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vecs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
